{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":840806,"sourceType":"datasetVersion","datasetId":59760}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n","metadata":{"editable":false}},{"cell_type":"markdown","source":"![animal](https://i.postimg.cc/fTyQm0Rm/Animals-VGG.gif)","metadata":{"editable":false}},{"cell_type":"markdown","source":"<h2>Summary üìù</h2>\n<div style=\"padding: 20px; border-color: #c77220; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border: 2px solid #0cb7f2;\">\n    <ul style=\"font-size: 18px; font-family: 'Arial'; line-height: 1.5em;\">\n        <li>In this project, we will develop an image classification model to categorize images into 10 different animal classes. We will use Python, TensorFlow, and Keras to implement a Convolutional Neural Network (CNN) that can accurately classify these images. The model will be trained and evaluated on a dataset containing various images of animals, and we will apply advanced techniques to ensure high performance and accuracy in the classification task.</li>\n    </ul>\n</div>","metadata":{"editable":false}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:15px; color:white; margin:0; font-size:100%; font-family:Pacifico; background-color:#0cb7f2; overflow:hidden\"><b> Import Libraries </b></div>","metadata":{"editable":false}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.cm import viridis\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout, Activation\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:15px; color:white; margin:0; font-size:100%; font-family:Pacifico; background-color:#0cb7f2; overflow:hidden\"><b> Processing Images </b></div>","metadata":{"editable":false}},{"cell_type":"markdown","source":"<div style=\"padding: 20px; border-color: #c77220; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border: 2px solid #0cb7f2;\">\n    <ul style=\"font-size: 18px; font-family: 'Arial'; line-height: 1.5em;\">\n        <li>In this section we are going to develop the resize of images, which is essential to ensure that all images have a consistent size (224x224 pixels) before feeding them into a neural network. This step is important because the neural networks require inputs with uniform shape to function correctly, and resizing also helps to reduce the computational load by standardizing the image dimensions, making the training process more efficient and improving model performance.</li>\n    </ul>\n</div>","metadata":{"editable":false}},{"cell_type":"code","source":"input_dir = '/kaggle/input/animals10/raw-img'\noutput_dir = '/kaggle/working/animals10-resized'\ntarget_size = (224, 224) \n\nos.makedirs(output_dir, exist_ok=True)\n\ndef resize_image(input_path, output_path, size):\n    with Image.open(input_path) as img:\n        img = img.resize(size)\n        img.save(output_path)\n\ndef process_directory(input_dir, output_dir, size):\n    total_files = 0\n    processed_files = 0\n\n    # Counting the total number of files to process\n    for root, dirs, files in os.walk(input_dir):\n        total_files += len(files)\n\n    \n    for root, dirs, files in os.walk(input_dir):\n        relative_path = os.path.relpath(root, input_dir)\n        output_folder = os.path.join(output_dir, relative_path)\n        os.makedirs(output_folder, exist_ok=True)\n\n        for file in files:\n            input_file_path = os.path.join(root, file)\n            output_file_path = os.path.join(output_folder, file)\n            try:\n                resize_image(input_file_path, output_file_path, size)\n                processed_files += 1\n            except Exception as e:\n                print(f\"Failed to process {input_file_path}: {e}\")\n\n    print(f\"Resizing completed: {processed_files}/{total_files} files processed.\")\n\nprocess_directory(input_dir, output_dir, target_size)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:15px; color:white; margin:0; font-size:100%; font-family:Pacifico; background-color:#0cb7f2; overflow:hidden\"><b> Data Visualization </b></div>","metadata":{"editable":false}},{"cell_type":"markdown","source":"<div style=\"padding: 20px; border-color: #c77220; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border: 2px solid #0cb7f2;\">\n    <ul style=\"font-size: 18px; font-family: 'Arial'; line-height: 1.5em;\">\n        <li>In this section we will use data visualization to gain a better understanding of the dataset by displaying the number of images belonging to each class. Additionally, we'll showcase a batch of example images from each class to provide a visual overview. </li>\n    </ul>\n</div>","metadata":{"editable":false}},{"cell_type":"code","source":"folder_path = '/kaggle/working/animals10-resized'\n\n# Translation dictionary from Italian to English\nclass_translation = {\n    \"cane\": \"dog\",\n    \"cavallo\": \"horse\",\n    \"elefante\": \"elephant\",\n    \"farfalla\": \"butterfly\",\n    \"gallina\": \"chicken\",\n    \"gatto\": \"cat\",\n    \"mucca\": \"cow\",\n    \"pecora\": \"sheep\",\n    \"scoiattolo\": \"squirrel\",\n    \"ragno\": \"spider\"\n}\n\n\nsubfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\ntranslated_subfolders = [class_translation[subfolder] for subfolder in subfolders]\n\n\nimage_counts = {class_translation[subfolder]: len(os.listdir(os.path.join(folder_path, subfolder))) for subfolder in subfolders}\n\ncolors = viridis(np.linspace(0, 1, len(image_counts)))\n\n\nplt.figure(figsize=(10, 6))\nplt.bar(image_counts.keys(), image_counts.values(), color=colors)\nplt.xlabel('Animal Class')\nplt.ylabel('Number of Images')\nplt.title('Number of Images in Each Animal Class')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\ntranslated_subfolders = {subfolder: class_translation[subfolder] for subfolder in subfolders}\n\n\nfig, axes = plt.subplots(4, 4, figsize=(12, 12))\nfig.suptitle('Sample Images from Different Animal Classes', fontsize=16)\n\nfor i, ax in enumerate(axes.flat):\n    selected_class = random.choice(list(translated_subfolders.keys()))\n    class_name = translated_subfolders[selected_class]\n    class_folder = os.path.join(folder_path, selected_class)\n    selected_image = random.choice(os.listdir(class_folder))\n    \n    img = image.load_img(os.path.join(class_folder, selected_image), target_size=(128, 128))\n    ax.imshow(img)\n    ax.axis('off')\n    \n    ax.set_title(class_name)\n\nplt.tight_layout()\nplt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding: 20px; border-color: #c77220; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border: 2px solid #0cb7f2;\">\n    <ul style=\"font-size: 18px; font-family: 'Arial'; line-height: 1.5em;\">\n        <li>We will also perform a data split, showing the distribution of images across each class in both the training and validation sets, helping to ensure a balanced dataset for model training. </li>\n    </ul>\n</div>","metadata":{"editable":false}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,  \n    #rotation_range=10,\n    #width_shift_range=0.1,\n    #height_shift_range=0.1,\n    #shear_range=0.1,\n    #zoom_range=0.1,\n    #horizontal_flip=True,\n    #fill_mode='nearest'\n)\n\ntrain_generator = datagen.flow_from_directory(\n    folder_path,\n    target_size=(224, 224),\n    batch_size=20,\n    class_mode='categorical',\n    subset='training'  \n)\n\nvalidation_generator = datagen.flow_from_directory(\n    folder_path,\n    target_size=(224, 224),\n    batch_size=20,\n    class_mode='categorical',\n    subset='validation'  \n)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_class_counts = np.bincount(train_generator.classes)\ntrain_class_names = [class_translation[class_name] for class_name in train_generator.class_indices.keys()]\n\n\nval_class_counts = np.bincount(validation_generator.classes)\nval_class_names = [class_translation[class_name] for class_name in validation_generator.class_indices.keys()]\n\n\ncolors_train = viridis(np.linspace(0, 1, len(train_class_names)))\nplt.figure(figsize=(10, 6))\nplt.bar(train_class_names, train_class_counts, color=colors_train)\nplt.title('Class Distribution in Training Set')\nplt.xlabel('Class')\nplt.ylabel('Number of Images')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n\ncolors_val = viridis(np.linspace(0, 1, len(val_class_names)))\nplt.figure(figsize=(10, 6))\nplt.bar(val_class_names, val_class_counts, color=colors_val)\nplt.title('Class Distribution in Validation Set')\nplt.xlabel('Class')\nplt.ylabel('Number of Images')\nplt.xticks(rotation=45, ha='right')\nplt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:15px; color:white; margin:0; font-size:100%; font-family:Pacifico; background-color:#0cb7f2; overflow:hidden\"><b> Build Model </b></div>","metadata":{"editable":false}},{"cell_type":"markdown","source":"<div style=\"padding: 20px; border-color: #c77220; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border: 2px solid #0cb7f2;\">\n    <ul style=\"font-size: 18px; font-family: 'Arial'; line-height: 1.5em;\">\n        <li>In this section we are going to build a model that uses the pre-trained VGG16 model as the base, which is loaded with ImageNet weights. The top layers of VGG16 are removed, and custom layers are added for the specific classification task. These custom layers include global average pooling, dense layers with batch normalization and dropout for regularization, and a final softmax layer for multi-class classification. The first 15 layers of the VGG16 model are set to be trainable, allowing fine-tuning, while the remaining layers are frozen to retain the pre-trained weights. The model is then compiled with the Adam optimizer, categorical cross-entropy loss, and accuracy as the evaluation metric.\n </li>\n    </ul>\n</div>","metadata":{"editable":false}},{"cell_type":"code","source":"base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Add custom top layers\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.3)(x)  \nx = Dense(512)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.2)(x) \npredictions = Dense(train_generator.num_classes, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\nfor layer in base_model.layers[:15]: \n    layer.trainable = True\nfor layer in base_model.layers[15:]:\n    layer.trainable = False\n\n\nmodel.compile(optimizer=Adam(learning_rate=1e-4),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(\n    'best_model.keras',         \n    monitor='val_accuracy',  \n    save_best_only=True,      \n    mode='max',              \n    verbose=1               \n)\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=80,            \n    epochs=150,\n    validation_data=validation_generator,\n    validation_steps=80,\n    callbacks=[checkpoint]        \n)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n\n# Set seed for reproducibility 42\nseed_value = 42\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)\nrandom.seed(seed_value)\n\nbest_model = load_model('best_model.keras')\n\ntest_loss, test_accuracy = best_model.evaluate(validation_generator, steps=80)\n\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Conclusions üìù</h2>\n<div style=\"padding: 20px; border-color: #c77220; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border: 2px solid #0cb7f2;\">\n    <ul style=\"font-size: 18px; font-family: 'Arial'; line-height: 1.5em;\">\n        <li>The image classification project successfully achieved an accuracy exceeding 90%, demonstrating the effectiveness of leveraging a pre-trained VGG16 model combined with custom top layers for fine-tuning. This approach not only harnesses the power of deep learning but also shows how transfer learning can significantly enhance performance in image classification tasks. The high accuracy underscores the robustness of this model in accurately classifying images across different categories.</li>\n    </ul>\n</div>","metadata":{"editable":false}},{"cell_type":"markdown","source":"# **Thank you for exploring this work!** \n\nIf you find it helpful, please consider upvoting it ‚ù§Ô∏è. \n\nYour support is greatly appreciated ü§©!","metadata":{"editable":false}}]}